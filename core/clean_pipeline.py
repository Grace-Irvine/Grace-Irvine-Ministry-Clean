#!/usr/bin/env python3
"""
æ•°æ®æ¸…æ´—ç®¡çº¿ä¸»è„šæœ¬
ä»åŸå§‹ Google Sheet è¯»å–ã€æ¸…æ´—ã€æ ¡éªŒå¹¶å†™å…¥æ¸…æ´—å±‚ Google Sheet
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd
from tqdm import tqdm

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir.parent))

from core.gsheet_utils import GSheetClient
from core.alias_utils import AliasMapper
from core.cleaning_rules import CleaningRules
from core.validators import DataValidator
from core.service_layer import ServiceLayerManager
from core.schema_manager import SchemaManager


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡çº¿"""
    
    # ç›®æ ‡è¾“å‡ºå­—æ®µé¡ºåº
    OUTPUT_SCHEMA = [
        'service_date', 'service_week', 'service_slot',
        'sermon_title', 'series', 'scripture',
        'preacher_id', 'preacher_name', 'preacher_department',
        'catechism',
        'reading_id', 'reading_name', 'reading_department',
        'worship_lead_id', 'worship_lead_name', 'worship_lead_department',
        'worship_team_1_id', 'worship_team_1_name', 'worship_team_1_department',
        'worship_team_2_id', 'worship_team_2_name', 'worship_team_2_department',
        'pianist_id', 'pianist_name', 'pianist_department',
        'songs',
        'audio_id', 'audio_name', 'audio_department',
        'video_id', 'video_name', 'video_department',
        'propresenter_play_id', 'propresenter_play_name', 'propresenter_play_department',
        'propresenter_update_id', 'propresenter_update_name', 'propresenter_update_department',
        'video_editor_id', 'video_editor_name', 'video_editor_department',
        'friday_child_ministry_id', 'friday_child_ministry_name', 'friday_child_ministry_department',
        'sunday_child_assistant_1_id', 'sunday_child_assistant_1_name', 'sunday_child_assistant_1_department',
        'sunday_child_assistant_2_id', 'sunday_child_assistant_2_name', 'sunday_child_assistant_2_department',
        'sunday_child_assistant_3_id', 'sunday_child_assistant_3_name', 'sunday_child_assistant_3_department',
        'newcomer_reception_1_id', 'newcomer_reception_1_name', 'newcomer_reception_1_department',
        'newcomer_reception_2_id', 'newcomer_reception_2_name', 'newcomer_reception_2_department',
        'friday_meal_id', 'friday_meal_name', 'friday_meal_department',
        'prayer_lead_id', 'prayer_lead_name', 'prayer_lead_department',
        'notes', 'source_row', 'updated_at'
    ]
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–æ¸…æ´—ç®¡çº¿
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.gsheet_client = GSheetClient()
        self.alias_mapper = AliasMapper()
        self.cleaning_rules = CleaningRules(self.config['cleaning_rules'])
        self.validator = DataValidator(self.config)
        self.schema_manager = SchemaManager(self.config)
        
        # å­˜å‚¨éƒ¨é—¨æ˜ å°„ä¿¡æ¯
        self.department_map = {}
        
        # åŠ è½½åˆ«åæ˜ å°„
        self._load_aliases()
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_aliases(self) -> None:
        """åŠ è½½äººååˆ«åæ˜ å°„"""
        alias_config = self.config.get('alias_sources', {}).get('people_alias_sheet')
        
        if not alias_config:
            logger.warning("æœªé…ç½®äººååˆ«åæ•°æ®æºï¼Œå°†ä½¿ç”¨é»˜è®¤ ID ç”Ÿæˆ")
            return
        
        try:
            self.alias_mapper.load_from_sheet(
                self.gsheet_client,
                alias_config['url'],
                alias_config['range']
            )
            stats = self.alias_mapper.get_stats()
            logger.info(
                f"æˆåŠŸåŠ è½½åˆ«åæ˜ å°„: {stats['total_aliases']} ä¸ªåˆ«å, "
                f"{stats['unique_persons']} ä¸ªå”¯ä¸€äººå‘˜"
            )
        except Exception as e:
            logger.warning(f"åŠ è½½åˆ«åæ˜ å°„å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤ ID ç”Ÿæˆ")
    
    def read_source_data(self) -> pd.DataFrame:
        """è¯»å–åŸå§‹æ•°æ®"""
        source_config = self.config['source_sheet']
        logger.info(f"è¯»å–åŸå§‹æ•°æ®: {source_config['range']}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–éƒ¨é—¨ä¿¡æ¯
        has_departments = 'departments' in self.config and self.config['departments']
        
        if has_departments:
            df, self.department_map = self.gsheet_client.read_range(
                source_config['url'],
                source_config['range'],
                return_department_info=True
            )
            logger.info(f"æ£€æµ‹åˆ°éƒ¨é—¨ä¿¡æ¯: {len(self.department_map)} åˆ—æœ‰éƒ¨é—¨æ ‡æ³¨")
        else:
            df = self.gsheet_client.read_range(
                source_config['url'],
                source_config['range']
            )
        
        logger.info(f"æˆåŠŸè¯»å– {len(df)} è¡ŒåŸå§‹æ•°æ®")
        
        # æ£€æµ‹æ–°åˆ—
        new_columns = self.schema_manager.detect_new_columns(df.columns.tolist())
        if new_columns:
            logger.warning(f"æ£€æµ‹åˆ° {len(new_columns)} ä¸ªæœªé…ç½®çš„åˆ—: {', '.join(new_columns)}")
            
            # ç”Ÿæˆé…ç½®å»ºè®®
            suggestions = self.schema_manager.generate_config_suggestions(
                new_columns, 
                self.department_map
            )
            
            # ä¿å­˜åˆ°æ—¥å¿—ç›®å½•
            suggestion_file = Path('logs/schema_suggestions.json')
            suggestion_file.parent.mkdir(exist_ok=True)
            with open(suggestion_file, 'w', encoding='utf-8') as f:
                json.dump(suggestions, f, ensure_ascii=False, indent=2)
            
            logger.info(f"é…ç½®å»ºè®®å·²ä¿å­˜åˆ°: {suggestion_file}")
        
        return df
    
    def clean_data(self, raw_df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ®
        
        Args:
            raw_df: åŸå§‹ DataFrame
            
        Returns:
            æ¸…æ´—åçš„ DataFrame
        """
        logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # 1. åˆ—åæ˜ å°„
        df = self._map_columns(raw_df)
        
        # 2. åº”ç”¨æ¸…æ´—è§„åˆ™
        cleaned_rows = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æ¸…æ´—æ•°æ®"):
            try:
                cleaned_row = self._clean_row(row, idx)
                cleaned_rows.append(cleaned_row)
            except Exception as e:
                logger.error(f"æ¸…æ´—ç¬¬ {idx + 2} è¡Œæ—¶å‡ºé”™: {e}")
                # ä»ç„¶æ·»åŠ éƒ¨åˆ†æ¸…æ´—çš„è¡Œï¼Œä½†æ ‡è®°é”™è¯¯
                cleaned_row = self._clean_row(row, idx, allow_errors=True)
                cleaned_row['notes'] = f"æ¸…æ´—é”™è¯¯: {str(e)}"
                cleaned_rows.append(cleaned_row)
        
        # 3. æ„å»ºè¾“å‡º DataFrame
        clean_df = pd.DataFrame(cleaned_rows)
        
        # 4. ç¡®ä¿è¾“å‡ºå­—æ®µé¡ºåº
        clean_df = self._ensure_schema(clean_df)
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…± {len(clean_df)} è¡Œ")
        
        # 5. è‡ªåŠ¨åŒæ­¥åˆ«åï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('alias_sources', {}).get('auto_sync', False):
            self._sync_aliases(clean_df)
        
        return clean_df
    
    def _map_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ˜ å°„åˆ—åï¼ˆä»ä¸­æ–‡åˆ°è‹±æ–‡æ ‡å‡†åï¼‰"""
        # ä½¿ç”¨ SchemaManager æ„å»ºåå‘æ˜ å°„
        reverse_mapping = {}
        
        for field_name, mapping in self.schema_manager.field_to_mapping_map.items():
            # å°†æ¯ä¸ªæºåˆ—åæ˜ å°„åˆ°æ ‡å‡†å­—æ®µå
            for source_col in mapping.sources:
                reverse_mapping[source_col] = field_name
        
        # é‡å‘½åå­˜åœ¨çš„åˆ—
        df = df.rename(columns=reverse_mapping)
        
        return df
    
    def _clean_row(
        self, 
        row: pd.Series, 
        idx: int, 
        allow_errors: bool = False
    ) -> Dict[str, Any]:
        """
        æ¸…æ´—å•è¡Œæ•°æ®
        
        Args:
            row: åŸå§‹è¡Œ
            idx: è¡Œç´¢å¼•
            allow_errors: æ˜¯å¦å…è®¸é”™è¯¯ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
            
        Returns:
            æ¸…æ´—åçš„è¡Œå­—å…¸
        """
        cleaned = {}
        
        # åŸºç¡€å­—æ®µ
        cleaned['source_row'] = idx + 2  # +2 å› ä¸ºç´¢å¼•ä»0å¼€å§‹ä¸”æœ‰è¡¨å¤´
        
        # æ—¥æœŸ
        service_date = self.cleaning_rules.clean_date(row.get('service_date'))
        cleaned['service_date'] = service_date or ''
        
        # æœåŠ¡å‘¨å’Œæ—¶æ®µ
        cleaned['service_week'] = self.cleaning_rules.get_service_week(service_date) if service_date else None
        cleaned['service_slot'] = self.cleaning_rules.infer_service_slot()
        
        # è®²é“ä¿¡æ¯
        cleaned['sermon_title'] = self.cleaning_rules.clean_text(row.get('sermon_title'))
        cleaned['series'] = self.cleaning_rules.clean_text(row.get('series'))
        cleaned['scripture'] = self.cleaning_rules.clean_scripture(row.get('scripture'))
        
        # è¦ç†é—®ç­”
        cleaned['catechism'] = self.cleaning_rules.clean_text(row.get('catechism'))
        
        # è¯»ç»ï¼ˆä½œä¸ºäººåå­—æ®µï¼Œå¸¦åˆ«åæ˜ å°„ï¼‰
        reading_name = self.cleaning_rules.clean_name(row.get('reading'))
        reading_id, reading_display = self.alias_mapper.resolve(reading_name)
        cleaned['reading_id'] = reading_id
        cleaned['reading_name'] = reading_display
        cleaned['reading_department'] = self.schema_manager.get_department('reading') or ''
        
        # è®²å‘˜ï¼ˆå¸¦åˆ«åæ˜ å°„ï¼‰
        preacher_name = self.cleaning_rules.clean_name(row.get('preacher'))
        preacher_id, preacher_display = self.alias_mapper.resolve(preacher_name)
        cleaned['preacher_id'] = preacher_id
        cleaned['preacher_name'] = preacher_display
        cleaned['preacher_department'] = self.schema_manager.get_department('preacher') or ''
        
        # æ•¬æ‹œå¸¦é¢†
        worship_lead_name = self.cleaning_rules.clean_name(row.get('worship_lead'))
        worship_lead_id, worship_lead_display = self.alias_mapper.resolve(worship_lead_name)
        cleaned['worship_lead_id'] = worship_lead_id
        cleaned['worship_lead_name'] = worship_lead_display
        cleaned['worship_lead_department'] = self.schema_manager.get_department('worship_lead') or ''
        
        # æ•¬æ‹œåŒå·¥1
        worship_team_1_name = self.cleaning_rules.clean_name(row.get('worship_team_1'))
        worship_team_1_id, worship_team_1_display = self.alias_mapper.resolve(worship_team_1_name)
        cleaned['worship_team_1_id'] = worship_team_1_id
        cleaned['worship_team_1_name'] = worship_team_1_display
        cleaned['worship_team_1_department'] = self.schema_manager.get_department('worship_team_1') or ''
        
        # æ•¬æ‹œåŒå·¥2
        worship_team_2_name = self.cleaning_rules.clean_name(row.get('worship_team_2'))
        worship_team_2_id, worship_team_2_display = self.alias_mapper.resolve(worship_team_2_name)
        cleaned['worship_team_2_id'] = worship_team_2_id
        cleaned['worship_team_2_name'] = worship_team_2_display
        cleaned['worship_team_2_department'] = self.schema_manager.get_department('worship_team_2') or ''
        
        # å¸ç´
        pianist_name = self.cleaning_rules.clean_name(row.get('pianist'))
        pianist_id, pianist_display = self.alias_mapper.resolve(pianist_name)
        cleaned['pianist_id'] = pianist_id
        cleaned['pianist_name'] = pianist_display
        cleaned['pianist_department'] = self.schema_manager.get_department('pianist') or ''
        
        # æ­Œæ›²ï¼ˆæ‹†åˆ†ä¸ºåˆ—è¡¨ï¼‰
        songs_list = self.cleaning_rules.split_songs(row.get('songs'))
        cleaned['songs'] = json.dumps(songs_list, ensure_ascii=False) if songs_list else ''
        
        # éŸ³æ§
        audio_name = self.cleaning_rules.clean_name(row.get('audio'))
        audio_id, audio_display = self.alias_mapper.resolve(audio_name)
        cleaned['audio_id'] = audio_id
        cleaned['audio_name'] = audio_display
        cleaned['audio_department'] = self.schema_manager.get_department('audio') or ''
        
        # å¯¼æ’­/æ‘„å½±
        video_name = self.cleaning_rules.clean_name(row.get('video'))
        video_id, video_display = self.alias_mapper.resolve(video_name)
        cleaned['video_id'] = video_id
        cleaned['video_name'] = video_display
        cleaned['video_department'] = self.schema_manager.get_department('video') or ''
        
        # ProPresenter æ’­æ”¾
        pp_play_name = self.cleaning_rules.clean_name(row.get('propresenter_play'))
        pp_play_id, pp_play_display = self.alias_mapper.resolve(pp_play_name)
        cleaned['propresenter_play_id'] = pp_play_id
        cleaned['propresenter_play_name'] = pp_play_display
        cleaned['propresenter_play_department'] = self.schema_manager.get_department('propresenter_play') or ''
        
        # ProPresenter æ›´æ–°
        pp_update_name = self.cleaning_rules.clean_name(row.get('propresenter_update'))
        pp_update_id, pp_update_display = self.alias_mapper.resolve(pp_update_name)
        cleaned['propresenter_update_id'] = pp_update_id
        cleaned['propresenter_update_name'] = pp_update_display
        cleaned['propresenter_update_department'] = self.schema_manager.get_department('propresenter_update') or ''
        
        # è§†é¢‘ç¼–è¾‘
        video_editor_name = self.cleaning_rules.clean_name(row.get('video_editor'))
        video_editor_id, video_editor_display = self.alias_mapper.resolve(video_editor_name)
        cleaned['video_editor_id'] = video_editor_id
        cleaned['video_editor_name'] = video_editor_display
        cleaned['video_editor_department'] = self.schema_manager.get_department('video_editor') or ''
        
        # å‘¨äº”è€å¸ˆ
        friday_child_ministry_name = self.cleaning_rules.clean_name(row.get('friday_child_ministry'))
        friday_child_ministry_id, friday_child_ministry_display = self.alias_mapper.resolve(friday_child_ministry_name)
        cleaned['friday_child_ministry_id'] = friday_child_ministry_id
        cleaned['friday_child_ministry_name'] = friday_child_ministry_display
        cleaned['friday_child_ministry_department'] = self.schema_manager.get_department('friday_child_ministry') or ''
        
        # å‘¨æ—¥åŠ©æ•™1
        sunday_child_assistant_1_name = self.cleaning_rules.clean_name(row.get('sunday_child_assistant_1'))
        sunday_child_assistant_1_id, sunday_child_assistant_1_display = self.alias_mapper.resolve(sunday_child_assistant_1_name)
        cleaned['sunday_child_assistant_1_id'] = sunday_child_assistant_1_id
        cleaned['sunday_child_assistant_1_name'] = sunday_child_assistant_1_display
        cleaned['sunday_child_assistant_1_department'] = self.schema_manager.get_department('sunday_child_assistant_1') or ''
        
        # å‘¨æ—¥åŠ©æ•™2
        sunday_child_assistant_2_name = self.cleaning_rules.clean_name(row.get('sunday_child_assistant_2'))
        sunday_child_assistant_2_id, sunday_child_assistant_2_display = self.alias_mapper.resolve(sunday_child_assistant_2_name)
        cleaned['sunday_child_assistant_2_id'] = sunday_child_assistant_2_id
        cleaned['sunday_child_assistant_2_name'] = sunday_child_assistant_2_display
        cleaned['sunday_child_assistant_2_department'] = self.schema_manager.get_department('sunday_child_assistant_2') or ''
        
        # å‘¨æ—¥åŠ©æ•™3
        sunday_child_assistant_3_name = self.cleaning_rules.clean_name(row.get('sunday_child_assistant_3'))
        sunday_child_assistant_3_id, sunday_child_assistant_3_display = self.alias_mapper.resolve(sunday_child_assistant_3_name)
        cleaned['sunday_child_assistant_3_id'] = sunday_child_assistant_3_id
        cleaned['sunday_child_assistant_3_name'] = sunday_child_assistant_3_display
        cleaned['sunday_child_assistant_3_department'] = self.schema_manager.get_department('sunday_child_assistant_3') or ''
        
        # æ–°äººæ¥å¾…1
        newcomer_reception_1_name = self.cleaning_rules.clean_name(row.get('newcomer_reception_1'))
        newcomer_reception_1_id, newcomer_reception_1_display = self.alias_mapper.resolve(newcomer_reception_1_name)
        cleaned['newcomer_reception_1_id'] = newcomer_reception_1_id
        cleaned['newcomer_reception_1_name'] = newcomer_reception_1_display
        cleaned['newcomer_reception_1_department'] = self.schema_manager.get_department('newcomer_reception_1') or ''
        
        # æ–°äººæ¥å¾…2
        newcomer_reception_2_name = self.cleaning_rules.clean_name(row.get('newcomer_reception_2'))
        newcomer_reception_2_id, newcomer_reception_2_display = self.alias_mapper.resolve(newcomer_reception_2_name)
        cleaned['newcomer_reception_2_id'] = newcomer_reception_2_id
        cleaned['newcomer_reception_2_name'] = newcomer_reception_2_display
        cleaned['newcomer_reception_2_department'] = self.schema_manager.get_department('newcomer_reception_2') or ''
                
        # é¥­é£Ÿç»„
        meal_group_name = self.cleaning_rules.clean_name(row.get('meal_group'))
        meal_group_id, meal_group_display = self.alias_mapper.resolve(meal_group_name)
        cleaned['meal_group_id'] = meal_group_id
        cleaned['meal_group_name'] = meal_group_display
        cleaned['meal_group_department'] = self.schema_manager.get_department('meal_group') or ''
        
        # å‘¨äº”é¥­é£Ÿ
        friday_meal_name = self.cleaning_rules.clean_name(row.get('friday_meal'))
        friday_meal_id, friday_meal_display = self.alias_mapper.resolve(friday_meal_name)
        cleaned['friday_meal_id'] = friday_meal_id
        cleaned['friday_meal_name'] = friday_meal_display
        cleaned['friday_meal_department'] = self.schema_manager.get_department('friday_meal') or ''
        
        # ç¥·å‘Šä¼šå¸¦é¢†
        prayer_lead_name = self.cleaning_rules.clean_name(row.get('prayer_lead'))
        prayer_lead_id, prayer_lead_display = self.alias_mapper.resolve(prayer_lead_name)
        cleaned['prayer_lead_id'] = prayer_lead_id
        cleaned['prayer_lead_name'] = prayer_lead_display
        cleaned['prayer_lead_department'] = self.schema_manager.get_department('prayer_lead') or ''
        
        # å¤‡æ³¨
        cleaned['notes'] = self.cleaning_rules.clean_text(row.get('notes', ''))
        
        # æ—¶é—´æˆ³
        cleaned['updated_at'] = datetime.utcnow().isoformat() + 'Z'
        
        return cleaned
    
    def _ensure_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿ DataFrame ç¬¦åˆè¾“å‡º Schema"""
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in self.OUTPUT_SCHEMA:
            if col not in df.columns:
                df[col] = ''
        
        # æŒ‰æŒ‡å®šé¡ºåºé‡æ’åˆ—
        df = df[self.OUTPUT_SCHEMA]
        
        return df
    
    def _sync_aliases(self, clean_df: pd.DataFrame) -> None:
        """
        è‡ªåŠ¨åŒæ­¥åˆ«ååˆ° Google Sheets
        
        Args:
            clean_df: æ¸…æ´—åçš„ DataFrame
        """
        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹è‡ªåŠ¨åŒæ­¥åˆ«å...")
            
            # è·å–é…ç½®
            alias_config = self.config.get('alias_sources', {})
            sheet_config = alias_config.get('people_alias_sheet', {})
            
            if not sheet_config:
                logger.warning("æœªé…ç½® people_alias_sheetï¼Œè·³è¿‡åˆ«ååŒæ­¥")
                return
            
            # è·å–è§’è‰²å­—æ®µåˆ—è¡¨
            role_fields = alias_config.get('role_fields', [
                'preacher', 'reading', 'worship_lead', 'worship_team_1', 'worship_team_2',
                'pianist', 'audio', 'video', 'propresenter_play', 'propresenter_update',
                'video_editor', 'friday_child_ministry', 'sunday_child_assistant_1', 'sunday_child_assistant_2', 'sunday_child_assistant_3',
                'newcomer_reception_1', 'newcomer_reception_2',
                'meal_group', 'friday_meal', 'prayer_lead'
            ])
            
            # 1. ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–æ‰€æœ‰äººååŠå…¶å‡ºç°æ¬¡æ•°
            names_counter = self.alias_mapper.extract_names_from_cleaned_data(
                clean_df, 
                role_fields
            )
            
            if not names_counter:
                logger.info("æœªæ‰¾åˆ°ä»»ä½•äººåï¼Œè·³è¿‡åŒæ­¥")
                return
            
            # 2. åŒæ­¥åˆ° Google Sheets
            url = sheet_config['url']
            range_name = sheet_config['range']
            
            stats = self.alias_mapper.sync_to_sheet(
                self.gsheet_client,
                url,
                range_name,
                names_counter
            )
            
            # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info("=" * 60)
            logger.info("âœ… åˆ«ååŒæ­¥å®Œæˆï¼")
            logger.info(f"   ğŸ“Š æ–°å¢: {stats['new_added']} ä¸ªåå­—")
            logger.info(f"   ğŸ“Š æ›´æ–°: {stats['updated']} ä¸ªåå­—çš„ç»Ÿè®¡")
            logger.info(f"   ğŸ“Š æ€»è®¡: {len(names_counter)} ä¸ªå”¯ä¸€äººå")
            logger.info("=" * 60)
            
            if stats['new_added'] > 0:
                logger.info("ğŸ’¡ æç¤º: è¯·åœ¨ Google Sheets ä¸­æ£€æŸ¥æ–°å¢çš„åå­—")
                logger.info("   1. åˆå¹¶åŒä¸€äººçš„ä¸åŒå†™æ³•ï¼ˆä¿®æ”¹ person_idï¼‰")
                logger.info("   2. è®¾ç½®ç»Ÿä¸€çš„ display_name")
                logger.info(f"   3. è¡¨æ ¼é“¾æ¥: {url}")
                logger.info("=" * 60)
        
        except Exception as e:
            logger.warning(f"åˆ«ååŒæ­¥å¤±è´¥: {e}", exc_info=True)
            logger.warning("æ³¨æ„ï¼šåˆ«ååŒæ­¥å¤±è´¥ä¸å½±å“æ•°æ®æ¸…æ´—æµç¨‹")
    
    def validate_data(self, df: pd.DataFrame):
        """æ ¡éªŒæ•°æ®"""
        logger.info("å¼€å§‹æ•°æ®æ ¡éªŒ...")
        report = self.validator.validate_dataframe(df)
        return report
    
    def write_output(self, df: pd.DataFrame, dry_run: bool = False) -> None:
        """
        å†™å…¥è¾“å‡º
        
        Args:
            df: æ¸…æ´—åçš„ DataFrame
            dry_run: æ˜¯å¦ä¸ºå¹²è·‘æ¨¡å¼ï¼ˆä»…è¾“å‡ºé¢„è§ˆï¼Œä¸å†™å› Sheetï¼‰
        """
        output_options = self.config['output_options']
        
        # è¾“å‡º CSV é¢„è§ˆ
        if output_options.get('emit_csv_preview'):
            csv_path = output_options['emit_csv_preview']
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"å·²ç”Ÿæˆ CSV é¢„è§ˆ: {csv_path}")
        
        # è¾“å‡º JSON é¢„è§ˆ
        if output_options.get('emit_json_preview'):
            json_path = output_options['emit_json_preview']
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            df.to_json(json_path, orient='records', force_ascii=False, indent=2)
            logger.info(f"å·²ç”Ÿæˆ JSON é¢„è§ˆ: {json_path}")
        
        # å†™å› Google Sheet
        if not dry_run:
            target_config = self.config['target_sheet']
            logger.info(f"å†™å…¥æ¸…æ´—å±‚ Google Sheet: {target_config['range']}")
            
            result = self.gsheet_client.write_range(
                target_config['url'],
                target_config['range'],
                df,
                include_header=True
            )
            
            logger.info(
                f"æˆåŠŸå†™å…¥ {result.get('updatedRows', 0)} è¡Œæ•°æ®åˆ°æ¸…æ´—å±‚"
            )
        else:
            logger.info("å¹²è·‘æ¨¡å¼ï¼šè·³è¿‡å†™å…¥ Google Sheet")
    
    def generate_service_layer(self, df: pd.DataFrame) -> Dict[str, Dict[str, Path]]:
        """
        ç”ŸæˆæœåŠ¡å±‚æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¹´ä»½ï¼‰
        
        Args:
            df: æ¸…æ´—åçš„ DataFrame
            
        Returns:
            æŒ‰å¹´ä»½ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        service_layer_config = self.config.get('service_layer', {})
        
        if not service_layer_config.get('enabled', False):
            logger.info("æœåŠ¡å±‚æœªå¯ç”¨ï¼Œè·³è¿‡")
            return {}
        
        logger.info("å¼€å§‹ç”ŸæˆæœåŠ¡å±‚æ•°æ®ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰...")
        
        # åˆå§‹åŒ–æœåŠ¡å±‚ç®¡ç†å™¨ï¼ˆä¼ å…¥ alias_mapper ä»¥ç¡®ä¿ generated json ä½¿ç”¨æœ€æ–°åˆ«åï¼‰
        manager = ServiceLayerManager(self.alias_mapper)
        
        # è·å–é…ç½®
        domains = service_layer_config.get('domains', ['sermon', 'volunteer', 'worship'])
        output_dir = Path(service_layer_config.get('local_output_dir', 'logs/service_layer'))
        
        # ç”Ÿæˆæ‰€æœ‰å¹´ä»½çš„æ•°æ®
        saved_files = manager.generate_all_years(df, output_dir, domains)
        
        # å¦‚æœé…ç½®äº† Cloud Storageï¼Œä¸Šä¼ åˆ° bucket
        storage_config = service_layer_config.get('storage', {})
        if storage_config.get('provider') == 'gcs' and storage_config.get('bucket'):
            try:
                from core.cloud_storage_utils import DomainStorageManager
                
                bucket_name = storage_config['bucket']
                base_path = storage_config.get('base_path', 'domains/')
                service_account_file = storage_config.get('service_account_file')
                
                logger.info(f"ä¸Šä¼ æœåŠ¡å±‚æ•°æ®åˆ° Cloud Storage: gs://{bucket_name}/{base_path}")
                
                storage_manager = DomainStorageManager(
                    bucket_name=bucket_name,
                    service_account_file=service_account_file,
                    base_path=base_path
                )
                
                # ä¸Šä¼ æ‰€æœ‰å¹´ä»½çš„æ•°æ®
                for year, year_files in saved_files.items():
                    logger.info(f"ä¸Šä¼  {year} æ•°æ®...")
                    for domain, file_path in year_files.items():
                        with open(file_path, 'r', encoding='utf-8') as f:
                            domain_data = json.load(f)
                        
                        # æ ¹æ®æ˜¯å¦ä¸º latest å†³å®šä¸Šä¼ è·¯å¾„
                        if year == 'latest':
                            # ç›´æ¥ä¸Šä¼  latest æ•°æ®ï¼Œå¼ºåˆ¶ä¸Šä¼ ä¸º latest.jsonï¼ˆé¿å…è‡ªåŠ¨æå–å¹´ä»½ï¼‰
                            uploaded = storage_manager.upload_domain_data(domain, domain_data, sync_latest=False, force_latest=True)
                        else:
                            # ä¸Šä¼ å¹´ä»½æ–‡ä»¶ï¼Œä¸ç«‹å³åŒæ­¥ï¼ˆé¿å…é‡å¤åŒæ­¥ï¼‰
                            uploaded = storage_manager.upload_domain_data(domain, domain_data, year=year, sync_latest=False)
                        
                        logger.info(f"  å·²ä¸Šä¼  {domain} ({year}): {uploaded}")
                
                # æ‰€æœ‰å¹´åº¦æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œç»Ÿä¸€åŒæ­¥ latest.json
                logger.info("å¼€å§‹åŒæ­¥ latest.json...")
                for domain in ['sermon', 'volunteer', 'worship']:
                    try:
                        storage_manager._sync_latest_from_yearly(domain)
                        logger.info(f"âœ… å·²åŒæ­¥ {domain}/latest.json")
                    except Exception as e:
                        logger.error(f"âŒ åŒæ­¥ {domain}/latest.json å¤±è´¥: {e}")
                
            except ImportError:
                logger.warning("google-cloud-storage æœªå®‰è£…ï¼Œè·³è¿‡äº‘å­˜å‚¨ä¸Šä¼ ")
            except Exception as e:
                logger.error(f"ä¸Šä¼ åˆ° Cloud Storage å¤±è´¥: {e}")
        
        return saved_files
    
    def run(self, dry_run: bool = False) -> int:
        """
        è¿è¡Œå®Œæ•´çš„æ¸…æ´—ç®¡çº¿
        
        Args:
            dry_run: æ˜¯å¦ä¸ºå¹²è·‘æ¨¡å¼
            
        Returns:
            é€€å‡ºç ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºæœ‰é”™è¯¯ï¼‰
        """
        try:
            # 1. è¯»å–åŸå§‹æ•°æ®
            raw_df = self.read_source_data()
            
            # 2. æ¸…æ´—æ•°æ®
            clean_df = self.clean_data(raw_df)
            
            # 3. æ ¡éªŒæ•°æ®
            report = self.validate_data(clean_df)
            
            # 4. è¾“å‡ºç»“æœ
            self.write_output(clean_df, dry_run=dry_run)
            
            # 5. ç”ŸæˆæœåŠ¡å±‚æ•°æ®
            service_files = self.generate_service_layer(clean_df)
            
            # 6. æ‰“å°æ‘˜è¦
            print("\n" + "=" * 60)
            print(report.format_report(max_issues=5))
            print("=" * 60)
            
            if service_files:
                print("\næœåŠ¡å±‚æ•°æ®ç”Ÿæˆ:")
                for year, year_files in service_files.items():
                    print(f"  {year}:")
                    for domain, file_path in year_files.items():
                        print(f"    {domain.upper()}: {file_path}")
            
            # 7. ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ—¥å¿—
            log_dir = Path('logs')
            log_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = log_dir / f'validation_report_{timestamp}.txt'
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report.format_report(max_issues=100))
            
            logger.info(f"è¯¦ç»†æ ¡éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # 7. è¿”å›é€€å‡ºç 
            if report.error_rows > 0:
                logger.error(f"æ¸…æ´—å®Œæˆä½†æœ‰ {report.error_rows} è¡Œå­˜åœ¨é”™è¯¯")
                return 1
            else:
                logger.info("æ¸…æ´—ç®¡çº¿æˆåŠŸå®Œæˆï¼")
                return 0
                
        except Exception as e:
            logger.error(f"æ¸…æ´—ç®¡çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return 1


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ•™ä¼šä¸»æ—¥äº‹å·¥æ•°æ®æ¸…æ´—ç®¡çº¿'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å¹²è·‘æ¨¡å¼ï¼šä»…è¾“å‡ºé¢„è§ˆï¼Œä¸å†™å› Google Sheet'
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œç®¡çº¿
    pipeline = CleaningPipeline(args.config)
    exit_code = pipeline.run(dry_run=args.dry_run)
    
    sys.exit(exit_code)


if __name__ == '__main__':
    main()

