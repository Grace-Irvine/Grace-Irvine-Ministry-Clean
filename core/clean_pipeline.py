#!/usr/bin/env python3
"""
æ•°æ®æ¸…æ´—ç®¡çº¿ä¸»è„šæœ¬
ä»åŸå§‹ Google Sheet è¯»å–ã€æ¸…æ´—ã€æ ¡éªŒå¹¶å†™å…¥æ¸…æ´—å±‚ Google Sheet
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd
from tqdm import tqdm

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir.parent))

from core.gsheet_utils import GSheetClient
from core.alias_utils import AliasMapper
from core.cleaning_rules import CleaningRules
from core.validators import DataValidator
from core.service_layer import ServiceLayerManager
from core.schema_manager import SchemaManager


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CleaningPipeline:
    """æ•°æ®æ¸…æ´—ç®¡çº¿"""
    
    BASE_FIELDS = {
        'service_date',
        'series',
        'sermon_title',
        'scripture',
        'catechism',
        'songs',
        'notes',
    }
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–æ¸…æ´—ç®¡çº¿
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.gsheet_client = GSheetClient()
        self.alias_mapper = AliasMapper()
        self.cleaning_rules = CleaningRules(self.config['cleaning_rules'])
        self.validator = DataValidator(self.config)
        self.schema_manager = SchemaManager(self.config)
        
        # å­˜å‚¨éƒ¨é—¨æ˜ å°„ä¿¡æ¯
        self.department_map = {}
        self.field_department_map = {}
        
        # åŠ¨æ€è§’è‰²å­—æ®µä¸è¾“å‡º Schema
        self.role_fields = self._build_role_fields()
        self.output_schema = self._build_output_schema()
        
        # åŠ è½½åˆ«åæ˜ å°„
        self._load_aliases()
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_aliases(self) -> None:
        """åŠ è½½äººååˆ«åæ˜ å°„"""
        alias_config = self.config.get('alias_sources', {}).get('people_alias_sheet')
        
        if not alias_config:
            logger.warning("æœªé…ç½®äººååˆ«åæ•°æ®æºï¼Œå°†ä½¿ç”¨é»˜è®¤ ID ç”Ÿæˆ")
            return
        
        try:
            self.alias_mapper.load_from_sheet(
                self.gsheet_client,
                alias_config['url'],
                alias_config['range']
            )
            stats = self.alias_mapper.get_stats()
            logger.info(
                f"æˆåŠŸåŠ è½½åˆ«åæ˜ å°„: {stats['total_aliases']} ä¸ªåˆ«å, "
                f"{stats['unique_persons']} ä¸ªå”¯ä¸€äººå‘˜"
            )
        except Exception as e:
            logger.warning(f"åŠ è½½åˆ«åæ˜ å°„å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤ ID ç”Ÿæˆ")

    def _build_role_fields(self) -> List[str]:
        """
        ä»é…ç½®ä¸­æ„å»ºè§’è‰²å­—æ®µåˆ—è¡¨ï¼ˆæŒ‰é…ç½®é¡ºåºï¼‰
        """
        role_fields = []
        for field_name in self.schema_manager.column_configs.keys():
            if field_name.startswith('_'):
                continue
            if field_name in self.BASE_FIELDS:
                continue
            role_fields.append(field_name)
        return role_fields

    def _build_output_schema(self) -> List[str]:
        """
        æ ¹æ®é…ç½®åŠ¨æ€ç”Ÿæˆè¾“å‡ºå­—æ®µé¡ºåº
        """
        schema: List[str] = []
        
        # å…ˆæ”¾æœåŠ¡æ—¥æœŸåŠè¡ç”Ÿå­—æ®µ
        if 'service_date' in self.schema_manager.column_configs:
            schema.extend(['service_date', 'service_week', 'service_slot'])
        
        for field_name in self.schema_manager.column_configs.keys():
            if field_name.startswith('_'):
                continue
            if field_name == 'service_date':
                continue
            
            if field_name in self.BASE_FIELDS:
                schema.append(field_name)
                continue
            
            schema.extend([
                f"{field_name}_id",
                f"{field_name}_name",
                f"{field_name}_department"
            ])
        
        # è¿½åŠ è¿è¡Œæ—¶å­—æ®µ
        for col in ['notes', 'source_row', 'updated_at']:
            if col not in schema:
                schema.append(col)
        
        return schema
    
    def read_source_data(self) -> pd.DataFrame:
        """è¯»å–åŸå§‹æ•°æ®"""
        source_config = self.config['source_sheet']
        logger.info(f"è¯»å–åŸå§‹æ•°æ®: {source_config['range']}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–éƒ¨é—¨ä¿¡æ¯
        has_departments = 'departments' in self.config and self.config['departments']
        
        if has_departments:
            df, self.department_map = self.gsheet_client.read_range(
                source_config['url'],
                source_config['range'],
                return_department_info=True
            )
            logger.info(f"æ£€æµ‹åˆ°éƒ¨é—¨ä¿¡æ¯: {len(self.department_map)} åˆ—æœ‰éƒ¨é—¨æ ‡æ³¨")
            
            # æ„å»ºæ ‡å‡†å­—æ®µååˆ°éƒ¨é—¨çš„æ˜ å°„
            self.field_department_map = {}
            for source_col, dept in self.department_map.items():
                field_name = self.schema_manager.get_standard_field_name(source_col)
                if field_name and dept:
                    self.field_department_map[field_name] = dept
        else:
            df = self.gsheet_client.read_range(
                source_config['url'],
                source_config['range']
            )
        
        logger.info(f"æˆåŠŸè¯»å– {len(df)} è¡ŒåŸå§‹æ•°æ®")
        
        # æ£€æµ‹æ–°åˆ—
        new_columns = self.schema_manager.detect_new_columns(df.columns.tolist())
        if new_columns:
            logger.warning(f"æ£€æµ‹åˆ° {len(new_columns)} ä¸ªæœªé…ç½®çš„åˆ—: {', '.join(new_columns)}")
            
            # ç”Ÿæˆé…ç½®å»ºè®®
            suggestions = self.schema_manager.generate_config_suggestions(
                new_columns, 
                self.department_map
            )
            
            # ä¿å­˜åˆ°æ—¥å¿—ç›®å½•
            suggestion_file = Path('logs/schema_suggestions.json')
            suggestion_file.parent.mkdir(exist_ok=True)
            with open(suggestion_file, 'w', encoding='utf-8') as f:
                json.dump(suggestions, f, ensure_ascii=False, indent=2)
            
            logger.info(f"é…ç½®å»ºè®®å·²ä¿å­˜åˆ°: {suggestion_file}")
        
        return df
    
    def clean_data(self, raw_df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ®
        
        Args:
            raw_df: åŸå§‹ DataFrame
            
        Returns:
            æ¸…æ´—åçš„ DataFrame
        """
        logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # 1. åˆ—åæ˜ å°„
        df = self._map_columns(raw_df)
        
        # 1.5 å¤„ç†é‡å¤åˆ—ï¼ˆåˆå¹¶ï¼‰
        df = self._merge_duplicate_columns(df)
        
        # 2. åº”ç”¨æ¸…æ´—è§„åˆ™
        cleaned_rows = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æ¸…æ´—æ•°æ®"):
            try:
                cleaned_row = self._clean_row(row, idx)
                cleaned_rows.append(cleaned_row)
            except Exception as e:
                logger.error(f"æ¸…æ´—ç¬¬ {idx + 2} è¡Œæ—¶å‡ºé”™: {e}")
                # ä»ç„¶æ·»åŠ éƒ¨åˆ†æ¸…æ´—çš„è¡Œï¼Œä½†æ ‡è®°é”™è¯¯
                cleaned_row = self._clean_row(row, idx, allow_errors=True)
                cleaned_row['notes'] = f"æ¸…æ´—é”™è¯¯: {str(e)}"
                cleaned_rows.append(cleaned_row)
        
        # 3. æ„å»ºè¾“å‡º DataFrame
        clean_df = pd.DataFrame(cleaned_rows)
        
        # 4. ç¡®ä¿è¾“å‡ºå­—æ®µé¡ºåº
        clean_df = self._ensure_schema(clean_df)
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…± {len(clean_df)} è¡Œ")
        
        # 5. è‡ªåŠ¨åŒæ­¥åˆ«åï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('alias_sources', {}).get('auto_sync', False):
            self._sync_aliases(clean_df)
        
        return clean_df
    
    def _map_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ˜ å°„åˆ—åï¼ˆä»ä¸­æ–‡åˆ°è‹±æ–‡æ ‡å‡†åï¼‰"""
        # ä½¿ç”¨ SchemaManager æ„å»ºåå‘æ˜ å°„
        reverse_mapping = {}
        
        for field_name, mapping in self.schema_manager.field_to_mapping_map.items():
            # å°†æ¯ä¸ªæºåˆ—åæ˜ å°„åˆ°æ ‡å‡†å­—æ®µå
            for source_col in mapping.sources:
                reverse_mapping[source_col] = field_name
        
        # é‡å‘½åå­˜åœ¨çš„åˆ—
        df = df.rename(columns=reverse_mapping)
        
        return df

    def _merge_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆå¹¶é‡å¤çš„åˆ—åï¼ˆä¼˜å…ˆå–éç©ºå€¼ï¼‰
        """
        if not df.columns.duplicated().any():
            return df
            
        logger.info("å‘ç°é‡å¤åˆ—åï¼Œå¼€å§‹åˆå¹¶...")
        unique_cols = df.columns.unique()
        
        new_data = {}
        
        for col in unique_cols:
            col_data = df[col]
            if isinstance(col_data, pd.DataFrame):
                logger.info(f"åˆå¹¶é‡å¤åˆ—: {col}")
                # å°†ç©ºå­—ç¬¦ä¸²æ›¿æ¢ä¸º None
                temp = col_data.copy()
                # ä½¿ç”¨ regex æ›¿æ¢ç©ºå­—ç¬¦ä¸²æˆ–ç©ºç™½å­—ç¬¦ä¸º None
                temp = temp.replace(r'^\s*$', None, regex=True)
                
                # ä½¿ç”¨ bfill(axis=1) å¡«å……
                combined = temp.bfill(axis=1).iloc[:, 0]
                
                # å¡«å……å›ç©ºå­—ç¬¦ä¸²
                combined = combined.fillna('')
                new_data[col] = combined
            else:
                new_data[col] = col_data
                
        return pd.DataFrame(new_data)
    
    def _clean_row(
        self, 
        row: pd.Series, 
        idx: int, 
        allow_errors: bool = False
    ) -> Dict[str, Any]:
        """
        æ¸…æ´—å•è¡Œæ•°æ®
        
        Args:
            row: åŸå§‹è¡Œ
            idx: è¡Œç´¢å¼•
            allow_errors: æ˜¯å¦å…è®¸é”™è¯¯ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
            
        Returns:
            æ¸…æ´—åçš„è¡Œå­—å…¸
        """
        cleaned = {}
        
        # åŸºç¡€å­—æ®µ
        cleaned['source_row'] = idx + 2  # +2 å› ä¸ºç´¢å¼•ä»0å¼€å§‹ä¸”æœ‰è¡¨å¤´
        
        # æ—¥æœŸ
        service_date = self.cleaning_rules.clean_date(row.get('service_date'))
        cleaned['service_date'] = service_date or ''
        
        # æœåŠ¡å‘¨å’Œæ—¶æ®µ
        cleaned['service_week'] = self.cleaning_rules.get_service_week(service_date) if service_date else None
        cleaned['service_slot'] = self.cleaning_rules.infer_service_slot()
        
        # è®²é“ä¿¡æ¯
        cleaned['sermon_title'] = self.cleaning_rules.clean_text(row.get('sermon_title'))
        cleaned['series'] = self.cleaning_rules.clean_text(row.get('series'))
        cleaned['scripture'] = self.cleaning_rules.clean_scripture(row.get('scripture'))
        
        # è¦ç†é—®ç­”
        cleaned['catechism'] = self.cleaning_rules.clean_text(row.get('catechism'))
        
        # æ­Œæ›²ï¼ˆæ‹†åˆ†ä¸ºåˆ—è¡¨ï¼‰
        songs_list = self.cleaning_rules.split_songs(row.get('songs'))
        cleaned['songs'] = json.dumps(songs_list, ensure_ascii=False) if songs_list else ''
        
        # è§’è‰²å­—æ®µï¼ˆå¸¦åˆ«åæ˜ å°„ï¼‰
        for role_field in self.role_fields:
            role_name = self.cleaning_rules.clean_name(row.get(role_field))
            role_id, role_display = self.alias_mapper.resolve(role_name)
            cleaned[f"{role_field}_id"] = role_id
            cleaned[f"{role_field}_name"] = role_display
            dept = self.schema_manager.get_department(role_field) or self.field_department_map.get(role_field, '')
            cleaned[f"{role_field}_department"] = dept
        
        # å¤‡æ³¨
        cleaned['notes'] = self.cleaning_rules.clean_text(row.get('notes', ''))
        
        # æ—¶é—´æˆ³
        cleaned['updated_at'] = datetime.utcnow().isoformat() + 'Z'
        
        return cleaned
    
    def _ensure_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿ DataFrame ç¬¦åˆè¾“å‡º Schema"""
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in self.output_schema:
            if col not in df.columns:
                df[col] = ''
        
        # æŒ‰æŒ‡å®šé¡ºåºé‡æ’åˆ—
        df = df[self.output_schema]
        
        return df
    
    def _sync_aliases(self, clean_df: pd.DataFrame) -> None:
        """
        è‡ªåŠ¨åŒæ­¥åˆ«ååˆ° Google Sheets
        
        Args:
            clean_df: æ¸…æ´—åçš„ DataFrame
        """
        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹è‡ªåŠ¨åŒæ­¥åˆ«å...")
            
            # è·å–é…ç½®
            alias_config = self.config.get('alias_sources', {})
            sheet_config = alias_config.get('people_alias_sheet', {})
            
            if not sheet_config:
                logger.warning("æœªé…ç½® people_alias_sheetï¼Œè·³è¿‡åˆ«ååŒæ­¥")
                return
            
            # è·å–è§’è‰²å­—æ®µåˆ—è¡¨
            role_fields = alias_config.get('role_fields')
            if role_fields:
                role_fields = list(dict.fromkeys(role_fields + self.role_fields))
            else:
                role_fields = self.role_fields
            
            # 1. ä»æ¸…æ´—åçš„æ•°æ®ä¸­æå–æ‰€æœ‰äººååŠå…¶å‡ºç°æ¬¡æ•°
            names_counter = self.alias_mapper.extract_names_from_cleaned_data(
                clean_df, 
                role_fields
            )
            
            if not names_counter:
                logger.info("æœªæ‰¾åˆ°ä»»ä½•äººåï¼Œè·³è¿‡åŒæ­¥")
                return
            
            # 2. åŒæ­¥åˆ° Google Sheets
            url = sheet_config['url']
            range_name = sheet_config['range']
            
            stats = self.alias_mapper.sync_to_sheet(
                self.gsheet_client,
                url,
                range_name,
                names_counter
            )
            
            # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info("=" * 60)
            logger.info("âœ… åˆ«ååŒæ­¥å®Œæˆï¼")
            logger.info(f"   ğŸ“Š æ–°å¢: {stats['new_added']} ä¸ªåå­—")
            logger.info(f"   ğŸ“Š æ›´æ–°: {stats['updated']} ä¸ªåå­—çš„ç»Ÿè®¡")
            logger.info(f"   ğŸ“Š æ€»è®¡: {len(names_counter)} ä¸ªå”¯ä¸€äººå")
            logger.info("=" * 60)
            
            if stats['new_added'] > 0:
                logger.info("ğŸ’¡ æç¤º: è¯·åœ¨ Google Sheets ä¸­æ£€æŸ¥æ–°å¢çš„åå­—")
                logger.info("   1. åˆå¹¶åŒä¸€äººçš„ä¸åŒå†™æ³•ï¼ˆä¿®æ”¹ person_idï¼‰")
                logger.info("   2. è®¾ç½®ç»Ÿä¸€çš„ display_name")
                logger.info(f"   3. è¡¨æ ¼é“¾æ¥: {url}")
                logger.info("=" * 60)
        
        except Exception as e:
            logger.warning(f"åˆ«ååŒæ­¥å¤±è´¥: {e}", exc_info=True)
            logger.warning("æ³¨æ„ï¼šåˆ«ååŒæ­¥å¤±è´¥ä¸å½±å“æ•°æ®æ¸…æ´—æµç¨‹")
    
    def validate_data(self, df: pd.DataFrame):
        """æ ¡éªŒæ•°æ®"""
        logger.info("å¼€å§‹æ•°æ®æ ¡éªŒ...")
        report = self.validator.validate_dataframe(df)
        return report
    
    def write_output(self, df: pd.DataFrame, dry_run: bool = False) -> None:
        """
        å†™å…¥è¾“å‡º
        
        Args:
            df: æ¸…æ´—åçš„ DataFrame
            dry_run: æ˜¯å¦ä¸ºå¹²è·‘æ¨¡å¼ï¼ˆä»…è¾“å‡ºé¢„è§ˆï¼Œä¸å†™å› Sheetï¼‰
        """
        output_options = self.config['output_options']
        
        # è¾“å‡º CSV é¢„è§ˆ
        if output_options.get('emit_csv_preview'):
            csv_path = output_options['emit_csv_preview']
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"å·²ç”Ÿæˆ CSV é¢„è§ˆ: {csv_path}")
        
        # è¾“å‡º JSON é¢„è§ˆ
        if output_options.get('emit_json_preview'):
            json_path = output_options['emit_json_preview']
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            df.to_json(json_path, orient='records', force_ascii=False, indent=2)
            logger.info(f"å·²ç”Ÿæˆ JSON é¢„è§ˆ: {json_path}")
        
        # å†™å› Google Sheet
        if not dry_run:
            target_config = self.config['target_sheet']
            logger.info(f"å†™å…¥æ¸…æ´—å±‚ Google Sheet: {target_config['range']}")
            
            result = self.gsheet_client.write_range(
                target_config['url'],
                target_config['range'],
                df,
                include_header=True
            )
            
            logger.info(
                f"æˆåŠŸå†™å…¥ {result.get('updatedRows', 0)} è¡Œæ•°æ®åˆ°æ¸…æ´—å±‚"
            )
        else:
            logger.info("å¹²è·‘æ¨¡å¼ï¼šè·³è¿‡å†™å…¥ Google Sheet")
    
    def generate_service_layer(self, df: pd.DataFrame) -> Dict[str, Dict[str, Path]]:
        """
        ç”ŸæˆæœåŠ¡å±‚æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰å¹´ä»½ï¼‰
        
        Args:
            df: æ¸…æ´—åçš„ DataFrame
            
        Returns:
            æŒ‰å¹´ä»½ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        service_layer_config = self.config.get('service_layer', {})
        
        if not service_layer_config.get('enabled', False):
            logger.info("æœåŠ¡å±‚æœªå¯ç”¨ï¼Œè·³è¿‡")
            return {}
        
        logger.info("å¼€å§‹ç”ŸæˆæœåŠ¡å±‚æ•°æ®ï¼ˆæ‰€æœ‰å¹´ä»½ï¼‰...")
        
        # åˆå§‹åŒ–æœåŠ¡å±‚ç®¡ç†å™¨ï¼ˆä¼ å…¥ alias_mapper ä»¥ç¡®ä¿ generated json ä½¿ç”¨æœ€æ–°åˆ«åï¼‰
        manager = ServiceLayerManager(self.alias_mapper)
        
        # è·å–é…ç½®
        domains = service_layer_config.get('domains', ['sermon', 'volunteer', 'worship'])
        output_dir = Path(service_layer_config.get('local_output_dir', 'logs/service_layer'))
        
        # ç”Ÿæˆæ‰€æœ‰å¹´ä»½çš„æ•°æ®
        saved_files = manager.generate_all_years(df, output_dir, domains)
        
        # å¦‚æœé…ç½®äº† Cloud Storageï¼Œä¸Šä¼ åˆ° bucket
        storage_config = service_layer_config.get('storage', {})
        if storage_config.get('provider') == 'gcs' and storage_config.get('bucket'):
            try:
                from core.cloud_storage_utils import DomainStorageManager
                
                bucket_name = storage_config['bucket']
                base_path = storage_config.get('base_path', 'domains/')
                service_account_file = storage_config.get('service_account_file')
                
                logger.info(f"ä¸Šä¼ æœåŠ¡å±‚æ•°æ®åˆ° Cloud Storage: gs://{bucket_name}/{base_path}")
                
                storage_manager = DomainStorageManager(
                    bucket_name=bucket_name,
                    service_account_file=service_account_file,
                    base_path=base_path
                )
                
                # ä¸Šä¼ æ‰€æœ‰å¹´ä»½çš„æ•°æ®
                for year, year_files in saved_files.items():
                    logger.info(f"ä¸Šä¼  {year} æ•°æ®...")
                    for domain, file_path in year_files.items():
                        with open(file_path, 'r', encoding='utf-8') as f:
                            domain_data = json.load(f)
                        
                        # æ ¹æ®æ˜¯å¦ä¸º latest å†³å®šä¸Šä¼ è·¯å¾„
                        if year == 'latest':
                            # ç›´æ¥ä¸Šä¼  latest æ•°æ®ï¼Œå¼ºåˆ¶ä¸Šä¼ ä¸º latest.jsonï¼ˆé¿å…è‡ªåŠ¨æå–å¹´ä»½ï¼‰
                            uploaded = storage_manager.upload_domain_data(domain, domain_data, sync_latest=False, force_latest=True)
                        else:
                            # ä¸Šä¼ å¹´ä»½æ–‡ä»¶ï¼Œä¸ç«‹å³åŒæ­¥ï¼ˆé¿å…é‡å¤åŒæ­¥ï¼‰
                            uploaded = storage_manager.upload_domain_data(domain, domain_data, year=year, sync_latest=False)
                        
                        logger.info(f"  å·²ä¸Šä¼  {domain} ({year}): {uploaded}")
                
                # æ‰€æœ‰å¹´åº¦æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œç»Ÿä¸€åŒæ­¥ latest.json
                logger.info("å¼€å§‹åŒæ­¥ latest.json...")
                for domain in ['sermon', 'volunteer', 'worship']:
                    try:
                        storage_manager._sync_latest_from_yearly(domain)
                        logger.info(f"âœ… å·²åŒæ­¥ {domain}/latest.json")
                    except Exception as e:
                        logger.error(f"âŒ åŒæ­¥ {domain}/latest.json å¤±è´¥: {e}")
                
            except ImportError:
                logger.warning("google-cloud-storage æœªå®‰è£…ï¼Œè·³è¿‡äº‘å­˜å‚¨ä¸Šä¼ ")
            except Exception as e:
                logger.error(f"ä¸Šä¼ åˆ° Cloud Storage å¤±è´¥: {e}")
        
        return saved_files
    
    def run(self, dry_run: bool = False) -> int:
        """
        è¿è¡Œå®Œæ•´çš„æ¸…æ´—ç®¡çº¿
        
        Args:
            dry_run: æ˜¯å¦ä¸ºå¹²è·‘æ¨¡å¼
            
        Returns:
            é€€å‡ºç ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œé 0 è¡¨ç¤ºæœ‰é”™è¯¯ï¼‰
        """
        try:
            # 1. è¯»å–åŸå§‹æ•°æ®
            raw_df = self.read_source_data()
            
            # 2. æ¸…æ´—æ•°æ®
            clean_df = self.clean_data(raw_df)
            
            # 3. æ ¡éªŒæ•°æ®
            report = self.validate_data(clean_df)
            
            # 4. è¾“å‡ºç»“æœ
            self.write_output(clean_df, dry_run=dry_run)
            
            # 5. ç”ŸæˆæœåŠ¡å±‚æ•°æ®
            service_files = self.generate_service_layer(clean_df)
            
            # 6. æ‰“å°æ‘˜è¦
            print("\n" + "=" * 60)
            print(report.format_report(max_issues=5))
            print("=" * 60)
            
            if service_files:
                print("\næœåŠ¡å±‚æ•°æ®ç”Ÿæˆ:")
                for year, year_files in service_files.items():
                    print(f"  {year}:")
                    for domain, file_path in year_files.items():
                        print(f"    {domain.upper()}: {file_path}")
            
            # 7. ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ—¥å¿—
            log_dir = Path('logs')
            log_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = log_dir / f'validation_report_{timestamp}.txt'
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report.format_report(max_issues=100))
            
            logger.info(f"è¯¦ç»†æ ¡éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # 7. è¿”å›é€€å‡ºç 
            if report.error_rows > 0:
                logger.error(f"æ¸…æ´—å®Œæˆä½†æœ‰ {report.error_rows} è¡Œå­˜åœ¨é”™è¯¯")
                return 1
            else:
                logger.info("æ¸…æ´—ç®¡çº¿æˆåŠŸå®Œæˆï¼")
                return 0
                
        except Exception as e:
            logger.error(f"æ¸…æ´—ç®¡çº¿æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return 1


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ•™ä¼šä¸»æ—¥äº‹å·¥æ•°æ®æ¸…æ´—ç®¡çº¿'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å¹²è·‘æ¨¡å¼ï¼šä»…è¾“å‡ºé¢„è§ˆï¼Œä¸å†™å› Google Sheet'
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œç®¡çº¿
    pipeline = CleaningPipeline(args.config)
    exit_code = pipeline.run(dry_run=args.dry_run)
    
    sys.exit(exit_code)


if __name__ == '__main__':
    main()
