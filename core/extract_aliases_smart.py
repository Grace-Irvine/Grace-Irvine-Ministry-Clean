#!/usr/bin/env python3
"""
æ™ºèƒ½æå–åˆ«åè„šæœ¬
ä»æœ¬åœ° Excel æ–‡ä»¶ä¸­æ™ºèƒ½æå–æ‰€æœ‰äººåå¹¶ç”Ÿæˆåˆ«å CSV
"""

import sys
import argparse
from pathlib import Path
from typing import Set, List, Tuple
import pandas as pd
from collections import Counter
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


def read_excel_with_merged_headers(excel_path: str) -> pd.DataFrame:
    """
    è¯»å–å¸¦åˆå¹¶è¡¨å¤´çš„ Excel æ–‡ä»¶
    ç¬¬ 0 è¡Œæ˜¯éƒ¨é—¨åˆ†ç±»ï¼Œç¬¬ 1 è¡Œæ˜¯å…·ä½“åˆ—å
    """
    print(f"ğŸ“– è¯»å– Excel æ–‡ä»¶: {excel_path}")
    
    # å…ˆè¯»å–åŸå§‹æ•°æ®
    df_raw = pd.read_excel(excel_path, header=None)
    
    # ç¬¬ 0 è¡Œæ˜¯éƒ¨é—¨ï¼Œç¬¬ 1 è¡Œæ˜¯åˆ—å
    departments = df_raw.iloc[0].fillna('')
    column_names = df_raw.iloc[1].fillna('')
    
    # åˆå¹¶ç”Ÿæˆå®Œæ•´åˆ—å
    full_column_names = []
    for dept, col in zip(departments, column_names):
        if col and col != '0':
            # å¦‚æœæœ‰å…·ä½“åˆ—åï¼Œä½¿ç”¨åˆ—å
            full_column_names.append(str(col))
        elif dept:
            # å¦åˆ™ä½¿ç”¨éƒ¨é—¨å
            full_column_names.append(str(dept))
        else:
            # éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ Unnamed
            full_column_names.append(f'Unnamed_{len(full_column_names)}')
    
    # ä»ç¬¬ 2 è¡Œå¼€å§‹æ˜¯æ•°æ®
    df = pd.read_excel(excel_path, header=None, skiprows=2)
    df.columns = full_column_names
    
    print(f"âœ… æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®")
    print(f"ğŸ“Š åˆ—å: {list(df.columns[:10])}...")
    print()
    
    return df


def extract_all_people(df: pd.DataFrame) -> List[Tuple[str, int]]:
    """
    ä» DataFrame ä¸­æå–æ‰€æœ‰äººå
    """
    print("ğŸ” æ‰«ææ‰€æœ‰åˆ—ï¼Œå¯»æ‰¾äººå...")
    
    # äººåç›¸å…³çš„å…³é”®è¯
    people_keywords = [
        'è®²å‘˜', 'å¸ä¼š', 'è¯»ç»', 'æ‹›å¾…', 'æ•¬æ‹œ', 'åŒå·¥', 'å¸ç´',
        'éŸ³æ§', 'å¯¼æ’­', 'æ‘„å½±', 'ProPresenter', 'æ’­æ”¾', 'æ›´æ–°',
        'åŠ©æ•™', 'ç¥·å‘Š', 'æœä¾', 'æ‰“æ‰«', 'å–é¥­', 'è´¢åŠ¡', 'åœºåœ°', 'åè°ƒ', 'å¤–å±•'
    ]
    
    # éœ€è¦æ’é™¤çš„å…³é”®è¯ï¼ˆè¿™äº›æ˜¯å†…å®¹ï¼Œä¸æ˜¯äººååˆ—ï¼‰
    exclude_keywords = [
        'æ—¥æœŸ', 'æ ‡é¢˜', 'ç»æ–‡', 'ç³»åˆ—', 'é—®ç­”', 'è©©æ­Œ', 'æ­Œå•', 'ç®€é¤'
    ]
    
    people_counter = Counter()
    processed_columns = []
    
    for col in df.columns:
        col_lower = str(col).lower()
        
        # è·³è¿‡æ˜æ˜¾ä¸æ˜¯äººåçš„åˆ—
        if any(keyword in col for keyword in exclude_keywords):
            continue
        
        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯äººååˆ—
        is_people_column = any(keyword in col for keyword in people_keywords)
        
        if is_people_column or 'åŒå·¥' in col or 'åè°ƒ' in col:
            processed_columns.append(col)
            print(f"  ğŸ“‹ å¤„ç†åˆ—: {col}")
            
            for value in df[col]:
                if pd.notna(value) and value not in ['0', 0, '']:
                    # æ¸…ç†äººå
                    cleaned = clean_name(str(value))
                    
                    # å¯èƒ½æ˜¯å¤šä¸ªäººåï¼Œç”¨é€—å·æˆ–ä¸­æ–‡é¡¿å·åˆ†éš”
                    names = re.split(r'[,ï¼Œã€;ï¼›]', cleaned)
                    
                    for name in names:
                        name = name.strip()
                        if name and len(name) > 1 and len(name) < 20:
                            # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯äººåçš„å†…å®¹
                            if not is_likely_name(name):
                                continue
                            people_counter[name] += 1
    
    print()
    print(f"âœ… å¤„ç†äº† {len(processed_columns)} ä¸ªåˆ—")
    print(f"âœ… å…±æ‰¾åˆ° {len(people_counter)} ä¸ªå”¯ä¸€äººå")
    print()
    
    # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    sorted_people = sorted(people_counter.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_people


def clean_name(name: str) -> str:
    """æ¸…ç†äººå"""
    # å»é™¤é¦–å°¾ç©ºæ ¼
    name = name.strip()
    
    # å»é™¤å¸¸è§çš„éäººåå­—ç¬¦
    name = name.replace('\n', ' ').replace('\r', '')
    
    # å»é™¤æ—¥æœŸæ ¼å¼ï¼ˆå¦‚ "9/26 æœµæœµ" -> "æœµæœµ"ï¼‰
    # åŒ¹é…å„ç§æ—¥æœŸæ ¼å¼ï¼š9/26, 9/26/2024, 9-26, 2024/9/26, 2024-9-26 ç­‰
    # æ—¥æœŸå¯èƒ½åœ¨å¼€å¤´ã€ä¸­é—´æˆ–æœ«å°¾
    date_patterns = [
        r'^\d{1,2}[/-]\d{1,2}[/-]?\d{0,4}\s+',  # 9/26, 9/26/2024, 9-26 ç­‰ï¼ˆå¼€å¤´ï¼‰
        r'\s+\d{1,2}[/-]\d{1,2}[/-]?\d{0,4}\s+',  # ä¸­é—´ä½ç½®çš„æ—¥æœŸ
        r'\s+\d{1,2}[/-]\d{1,2}[/-]?\d{0,4}$',  # æœ«å°¾ä½ç½®çš„æ—¥æœŸ
        r'^\d{4}[/-]\d{1,2}[/-]\d{1,2}\s+',  # 2024/9/26 ç­‰ï¼ˆå¼€å¤´ï¼‰
        r'\s+\d{4}[/-]\d{1,2}[/-]\d{1,2}\s+',  # ä¸­é—´ä½ç½®çš„å®Œæ•´æ—¥æœŸ
        r'\s+\d{4}[/-]\d{1,2}[/-]\d{1,2}$',  # æœ«å°¾ä½ç½®çš„å®Œæ•´æ—¥æœŸ
    ]
    
    for pattern in date_patterns:
        name = re.sub(pattern, ' ', name)
    
    # å»é™¤å¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name)
    name = name.strip()
    
    return name


def is_likely_name(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯äººå"""
    # å¤ªçŸ­æˆ–å¤ªé•¿çš„ä¸å¤ªå¯èƒ½æ˜¯äººå
    if len(text) < 2 or len(text) > 20:
        return False
    
    # æ’é™¤çº¯æ•°å­—
    if text.isdigit():
        return False
    
    # æ’é™¤å¸¸è§çš„éäººåå†…å®¹
    exclude_patterns = [
        r'^\d+$',  # çº¯æ•°å­—
        r'^ç¬¬\d+',  # ç¬¬N...
        r'æœˆ\d+æ—¥',  # æ—¥æœŸ
        r'^\d{4}',  # å¹´ä»½
    ]
    
    for pattern in exclude_patterns:
        if re.match(pattern, text):
            return False
    
    return True


def generate_alias_csv(
    people_list: List[Tuple[str, int]],
    output_path: str,
    include_count: bool = True
):
    """ç”Ÿæˆåˆ«å CSV æ–‡ä»¶"""
    print(f"ğŸ“ ç”Ÿæˆåˆ«å CSV: {output_path}")
    
    # å‡†å¤‡æ•°æ®
    rows = []
    for name, count in people_list:
        # ç”Ÿæˆé»˜è®¤çš„ person_id
        # ä½¿ç”¨ç®€å•çš„è§„åˆ™ï¼Œç”¨æˆ·å¯ä»¥åç»­æ‰‹åŠ¨è°ƒæ•´
        person_id = generate_person_id(name)
        
        row = {
            'alias': name,
            'person_id': person_id,
            'display_name': name
        }
        
        if include_count:
            row['count'] = count
            row['note'] = ''  # ç”¨æˆ·å¯ä»¥æ·»åŠ å¤‡æ³¨
        
        rows.append(row)
    
    # åˆ›å»º DataFrame
    df = pd.DataFrame(rows)
    
    # ä¿å­˜ä¸º CSV
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å·²ç”Ÿæˆ {len(rows)} æ¡åˆ«åè®°å½•")
    print()
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("ğŸ“Š å‡ºç°é¢‘ç‡æœ€é«˜çš„å‰ 20 ä½:")
    top_20 = df.nlargest(min(20, len(df)), 'count') if include_count else df.head(20)
    for i, (_, row) in enumerate(top_20.iterrows(), 1):
        if include_count:
            print(f"  {i:2d}. {row['display_name']:20s} - {row['count']:2d} æ¬¡")
        else:
            print(f"  {i:2d}. {row['display_name']}")
    print()
    
    print("ğŸ’¡ æç¤º:")
    print("  1. è¯·æ£€æŸ¥å¹¶æ‰‹åŠ¨ç¼–è¾‘ CSV æ–‡ä»¶")
    print("  2. åˆå¹¶åŒä¸€äººçš„ä¸åŒå†™æ³•ï¼ˆå¦‚ï¼šå¼ ç«‹å†›ã€Zhang Lijunï¼‰")
    print("  3. å°†å®ƒä»¬çš„ person_id æ”¹ä¸ºç›¸åŒå€¼")
    print("  4. è®¾ç½®ç»Ÿä¸€çš„ display_name")
    print("  5. åœ¨ note åˆ—æ·»åŠ å¤‡æ³¨ï¼ˆå¦‚ï¼šç‰§å¸ˆã€é•¿è€ç­‰ï¼‰")
    print()


def generate_person_id(name: str) -> str:
    """ç”Ÿæˆé»˜è®¤çš„ person_id"""
    # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    cleaned = re.sub(r'[^\w]', '', name.lower())
    
    # å¦‚æœå…¨æ˜¯è‹±æ–‡ï¼Œç›´æ¥ä½¿ç”¨
    if cleaned.isascii():
        return f"person_{cleaned}"
    
    # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨æ‹¼éŸ³æˆ–ç®€å•ç¼–ç 
    # è¿™é‡Œä½¿ç”¨ç®€å•çš„æ–¹æ¡ˆï¼Œç”¨æˆ·åç»­å¯ä»¥æ‰‹åŠ¨è°ƒæ•´
    return f"person_{hash(name) % 10000:04d}_{cleaned[:10]}"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä» Excel æ–‡ä»¶æ™ºèƒ½æå–äººåå¹¶ç”Ÿæˆåˆ«å CSV',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ç”Ÿæˆåˆ«å CSVï¼ˆåŒ…å«å‡ºç°æ¬¡æ•°å’Œå¤‡æ³¨åˆ—ï¼‰
  python scripts/extract_aliases_smart.py --excel tests/data.xlsx
  
  # æŒ‡å®šè¾“å‡ºè·¯å¾„
  python scripts/extract_aliases_smart.py --excel tests/data.xlsx -o tests/my_aliases.csv
  
  # ç”Ÿæˆç”¨äºä¸Šä¼ åˆ° Google Sheets çš„ç‰ˆæœ¬ï¼ˆä¸å«æ¬¡æ•°ï¼‰
  python scripts/extract_aliases_smart.py --excel tests/data.xlsx -o tests/aliases_for_sheets.csv --no-count
        """
    )
    
    parser.add_argument(
        '--excel',
        type=str,
        required=True,
        help='Excel æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '-o', '--output',
        type=str,
        default='tests/generated_aliases.csv',
        help='è¾“å‡º CSV æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: tests/generated_aliases.csvï¼‰'
    )
    parser.add_argument(
        '--no-count',
        action='store_true',
        help='ä¸åœ¨ CSV ä¸­åŒ…å«å‡ºç°æ¬¡æ•°åˆ—ï¼ˆç”¨äºä¸Šä¼ åˆ° Google Sheetsï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.excel).exists():
        print(f"âŒ é”™è¯¯: Excel æ–‡ä»¶ä¸å­˜åœ¨: {args.excel}")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸ¯ æ™ºèƒ½äººåæå–å·¥å…·")
    print("=" * 60)
    print()
    
    # è¯»å– Excel
    try:
        df = read_excel_with_merged_headers(args.excel)
    except Exception as e:
        print(f"âŒ é”™è¯¯: è¯»å– Excel å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # æå–äººå
    try:
        people_list = extract_all_people(df)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æå–äººåå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    if not people_list:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•äººå")
        sys.exit(0)
    
    # ç”Ÿæˆ CSV
    try:
        generate_alias_csv(
            people_list,
            args.output,
            include_count=not args.no_count
        )
    except Exception as e:
        print(f"âŒ é”™è¯¯: ç”Ÿæˆ CSV å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print(f"ğŸ‰ å®Œæˆï¼åˆ«å CSV å·²ä¿å­˜åˆ°: {args.output}")
    print()
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print(f"  1. æ‰“å¼€å¹¶ç¼–è¾‘ {args.output}")
    print("  2. åˆå¹¶åŒä¸€äººçš„ä¸åŒåˆ«åï¼ˆä¿®æ”¹ person_id ä¸ºç›¸åŒå€¼ï¼‰")
    print("  3. æµ‹è¯•æœ¬åœ°æ¸…æ´—: python scripts/debug_clean_local.py --excel tests/data.xlsx --aliases " + args.output)
    print()


if __name__ == '__main__':
    main()

