#!/usr/bin/env python3
"""
æœ¬åœ°è°ƒè¯•æ¸…æ´—è„šæœ¬
ä»æœ¬åœ° Excel æ–‡ä»¶è¯»å–æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œä¸è¿æ¥ Google Sheets
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.alias_utils import AliasMapper
from scripts.cleaning_rules import CleaningRules
from scripts.validators import DataValidator


class LocalCleaningPipeline:
    """æœ¬åœ°æ¸…æ´—ç®¡çº¿ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    
    # ç›®æ ‡è¾“å‡ºå­—æ®µé¡ºåº
    OUTPUT_SCHEMA = [
        'service_date', 'service_week', 'service_slot',
        'sermon_title', 'series', 'scripture',
        'preacher_id', 'preacher_name',
        'catechism', 'reading',
        'worship_lead_id', 'worship_lead_name',
        'worship_team_ids', 'worship_team_names',
        'pianist_id', 'pianist_name',
        'songs',
        'audio_id', 'audio_name',
        'video_id', 'video_name',
        'propresenter_play_id', 'propresenter_play_name',
        'propresenter_update_id', 'propresenter_update_name',
        'assistant_id', 'assistant_name',
        'notes', 'source_row', 'updated_at'
    ]
    
    def __init__(self, config_path: str, alias_csv: str = None):
        """
        åˆå§‹åŒ–æœ¬åœ°æ¸…æ´—ç®¡çº¿
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            alias_csv: åˆ«å CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.config = self._load_config(config_path)
        self.alias_mapper = AliasMapper()
        self.cleaning_rules = CleaningRules(self.config['cleaning_rules'])
        self.validator = DataValidator(self.config)
        
        # åŠ è½½åˆ«åæ˜ å°„
        if alias_csv and Path(alias_csv).exists():
            self.alias_mapper.load_from_csv(alias_csv)
            stats = self.alias_mapper.get_stats()
            print(f"âœ… åŠ è½½åˆ«å: {stats['total_aliases']} ä¸ªåˆ«å, {stats['unique_persons']} ä¸ªäººå‘˜")
        else:
            print("âš ï¸  æœªåŠ è½½åˆ«åæ˜ å°„ï¼Œå°†ä½¿ç”¨é»˜è®¤ ID ç”Ÿæˆ")
    
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def read_excel(self, excel_path: str) -> pd.DataFrame:
        """è¯»å– Excel æ–‡ä»¶"""
        print(f"ğŸ“– è¯»å– Excel: {excel_path}")
        df = pd.read_excel(excel_path)
        print(f"âœ… è¯»å– {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        return df
    
    def _map_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ˜ å°„åˆ—åï¼ˆä»ä¸­æ–‡åˆ°è‹±æ–‡ï¼‰"""
        column_mapping = self.config['columns']
        reverse_mapping = {v: k for k, v in column_mapping.items()}
        df = df.rename(columns=reverse_mapping)
        return df
    
    def clean_data(self, raw_df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        print("\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # 1. åˆ—åæ˜ å°„
        df = self._map_columns(raw_df)
        
        # 2. åº”ç”¨æ¸…æ´—è§„åˆ™
        cleaned_rows = []
        for idx, row in df.iterrows():
            try:
                cleaned_row = self._clean_row(row, idx)
                cleaned_rows.append(cleaned_row)
            except Exception as e:
                print(f"âš ï¸  ç¬¬ {idx + 2} è¡Œæ¸…æ´—å¤±è´¥: {e}")
                cleaned_row = self._clean_row(row, idx, allow_errors=True)
                cleaned_row['notes'] = f"æ¸…æ´—é”™è¯¯: {str(e)}"
                cleaned_rows.append(cleaned_row)
        
        # 3. æ„å»ºè¾“å‡º DataFrame
        clean_df = pd.DataFrame(cleaned_rows)
        
        # 4. ç¡®ä¿è¾“å‡ºå­—æ®µé¡ºåº
        clean_df = self._ensure_schema(clean_df)
        
        print(f"âœ… æ¸…æ´—å®Œæˆ: {len(clean_df)} è¡Œ")
        return clean_df
    
    def _clean_row(self, row: pd.Series, idx: int, allow_errors: bool = False) -> dict:
        """æ¸…æ´—å•è¡Œæ•°æ®"""
        cleaned = {}
        
        # åŸºç¡€å­—æ®µ
        cleaned['source_row'] = idx + 2
        
        # æ—¥æœŸ
        service_date = self.cleaning_rules.clean_date(row.get('service_date'))
        cleaned['service_date'] = service_date or ''
        
        # æœåŠ¡å‘¨å’Œæ—¶æ®µ
        cleaned['service_week'] = self.cleaning_rules.get_service_week(service_date) if service_date else None
        cleaned['service_slot'] = self.cleaning_rules.infer_service_slot()
        
        # è®²é“ä¿¡æ¯
        cleaned['sermon_title'] = self.cleaning_rules.clean_text(row.get('sermon_title'))
        cleaned['series'] = self.cleaning_rules.clean_text(row.get('series'))
        cleaned['scripture'] = self.cleaning_rules.clean_scripture(row.get('scripture'))
        
        # è¦ç†é—®ç­”å’Œè¯»ç»
        cleaned['catechism'] = self.cleaning_rules.clean_text(row.get('catechism'))
        cleaned['reading'] = self.cleaning_rules.clean_text(row.get('reading'))
        
        # è®²å‘˜
        preacher_name = self.cleaning_rules.clean_name(row.get('preacher'))
        preacher_id, preacher_display = self.alias_mapper.resolve(preacher_name)
        cleaned['preacher_id'] = preacher_id
        cleaned['preacher_name'] = preacher_display
        
        # æ•¬æ‹œå¸¦é¢†
        worship_lead_name = self.cleaning_rules.clean_name(row.get('worship_lead'))
        worship_lead_id, worship_lead_display = self.alias_mapper.resolve(worship_lead_name)
        cleaned['worship_lead_id'] = worship_lead_id
        cleaned['worship_lead_name'] = worship_lead_display
        
        # æ•¬æ‹œåŒå·¥
        worship_team_names = self.cleaning_rules.merge_columns(row, ['worship_team_1', 'worship_team_2'])
        worship_team_ids, worship_team_displays = self.alias_mapper.resolve_list(worship_team_names)
        cleaned['worship_team_ids'] = json.dumps(worship_team_ids, ensure_ascii=False) if worship_team_ids else ''
        cleaned['worship_team_names'] = json.dumps(worship_team_displays, ensure_ascii=False) if worship_team_displays else ''
        
        # å¸ç´
        pianist_name = self.cleaning_rules.clean_name(row.get('pianist'))
        pianist_id, pianist_display = self.alias_mapper.resolve(pianist_name)
        cleaned['pianist_id'] = pianist_id
        cleaned['pianist_name'] = pianist_display
        
        # æ­Œæ›²
        songs_list = self.cleaning_rules.split_songs(row.get('songs'))
        cleaned['songs'] = json.dumps(songs_list, ensure_ascii=False) if songs_list else ''
        
        # éŸ³æ§
        audio_name = self.cleaning_rules.clean_name(row.get('audio'))
        audio_id, audio_display = self.alias_mapper.resolve(audio_name)
        cleaned['audio_id'] = audio_id
        cleaned['audio_name'] = audio_display
        
        # å¯¼æ’­/æ‘„å½±
        video_name = self.cleaning_rules.clean_name(row.get('video'))
        video_id, video_display = self.alias_mapper.resolve(video_name)
        cleaned['video_id'] = video_id
        cleaned['video_name'] = video_display
        
        # ProPresenter æ’­æ”¾
        pp_play_name = self.cleaning_rules.clean_name(row.get('propresenter_play'))
        pp_play_id, pp_play_display = self.alias_mapper.resolve(pp_play_name)
        cleaned['propresenter_play_id'] = pp_play_id
        cleaned['propresenter_play_name'] = pp_play_display
        
        # ProPresenter æ›´æ–°
        pp_update_name = self.cleaning_rules.clean_name(row.get('propresenter_update'))
        pp_update_id, pp_update_display = self.alias_mapper.resolve(pp_update_name)
        cleaned['propresenter_update_id'] = pp_update_id
        cleaned['propresenter_update_name'] = pp_update_display
        
        # åŠ©æ•™
        assistant_name = self.cleaning_rules.clean_name(row.get('assistant'))
        assistant_id, assistant_display = self.alias_mapper.resolve(assistant_name)
        cleaned['assistant_id'] = assistant_id
        cleaned['assistant_name'] = assistant_display
        
        # å¤‡æ³¨
        cleaned['notes'] = self.cleaning_rules.clean_text(row.get('notes', ''))
        
        # æ—¶é—´æˆ³
        cleaned['updated_at'] = datetime.utcnow().isoformat() + 'Z'
        
        return cleaned
    
    def _ensure_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿ DataFrame ç¬¦åˆè¾“å‡º Schema"""
        for col in self.OUTPUT_SCHEMA:
            if col not in df.columns:
                df[col] = ''
        df = df[self.OUTPUT_SCHEMA]
        return df
    
    def validate_data(self, df: pd.DataFrame):
        """æ ¡éªŒæ•°æ®"""
        print("\nâœ… å¼€å§‹æ•°æ®æ ¡éªŒ...")
        report = self.validator.validate_dataframe(df)
        return report
    
    def save_output(self, df: pd.DataFrame, csv_path: str, json_path: str = None):
        """ä¿å­˜è¾“å‡º"""
        print(f"\nğŸ’¾ ä¿å­˜è¾“å‡º...")
        
        # ä¿å­˜ CSV
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… CSV: {csv_path}")
        
        # ä¿å­˜ JSON
        if json_path:
            df.to_json(json_path, orient='records', force_ascii=False, indent=2)
            print(f"  âœ… JSON: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æœ¬åœ°è°ƒè¯•æ¸…æ´—è„šæœ¬ï¼ˆä» Excel è¯»å–ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ç”¨æ³•
  python scripts/debug_clean_local.py --excel tests/data.xlsx
  
  # ä½¿ç”¨åˆ«å CSV
  python scripts/debug_clean_local.py --excel tests/data.xlsx --aliases tests/aliases.csv
  
  # æŒ‡å®šè¾“å‡ºè·¯å¾„
  python scripts/debug_clean_local.py --excel tests/data.xlsx -o tests/output.csv
        """
    )
    
    parser.add_argument(
        '--excel',
        type=str,
        required=True,
        help='Excel æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--aliases',
        type=str,
        default='tests/generated_aliases.csv',
        help='åˆ«å CSV æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '-o', '--output',
        type=str,
        default='tests/debug_clean_output.csv',
        help='è¾“å‡º CSV æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--json',
        type=str,
        default='tests/debug_clean_output.json',
        help='è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶
    if not Path(args.excel).exists():
        print(f"âŒ Excel æ–‡ä»¶ä¸å­˜åœ¨: {args.excel}")
        sys.exit(1)
    
    if not Path(args.config).exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸ”§ æœ¬åœ°è°ƒè¯•æ¸…æ´—è„šæœ¬")
    print("=" * 60)
    print()
    
    # åˆ›å»ºç®¡çº¿
    pipeline = LocalCleaningPipeline(args.config, args.aliases)
    
    # è¯»å–æ•°æ®
    raw_df = pipeline.read_excel(args.excel)
    
    # æ¸…æ´—æ•°æ®
    clean_df = pipeline.clean_data(raw_df)
    
    # æ ¡éªŒæ•°æ®
    report = pipeline.validate_data(clean_df)
    
    # ä¿å­˜è¾“å‡º
    pipeline.save_output(clean_df, args.output, args.json)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print(report.format_report(max_issues=10))
    print("=" * 60)
    
    print("\nğŸ‰ è°ƒè¯•å®Œæˆï¼")
    print(f"\nğŸ“Š è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {args.output}")
    if args.json:
        print(f"  - {args.json}")
    print()
    
    return 0 if report.error_rows == 0 else 1


if __name__ == '__main__':
    sys.exit(main())

