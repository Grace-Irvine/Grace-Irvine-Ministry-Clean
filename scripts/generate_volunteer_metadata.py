#!/usr/bin/env python3
"""
ä»åˆ«åè¡¨ç”ŸæˆåŒå·¥å…ƒæ•°æ®è¡¨
è‡ªåŠ¨å¡«å…… person_id, person_name, updated_at ä¸‰åˆ—
å…¶ä»–åˆ—ï¼ˆfamily_group, unavailable_start, unavailable_end, unavailable_reason, notesï¼‰ç”±ç”¨æˆ·æ‰‹åŠ¨å¡«å†™
"""

import json
import logging
from datetime import datetime
from pathlib import Path
import pandas as pd
import sys

# æ·»åŠ è„šæœ¬ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from gsheet_utils import GSheetClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path='config/config.json'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def read_alias_data(config):
    """ä»åˆ«åè¡¨è¯»å–æ•°æ®"""
    alias_config = config['alias_sources']['people_alias_sheet']
    
    logger.info(f"è¯»å–åˆ«åè¡¨: {alias_config['url']}")
    
    # ä½¿ç”¨ GSheetClient
    client = GSheetClient()
    alias_df = client.read_range(alias_config['url'], alias_config['range'])
    
    logger.info(f"åˆ«åè¡¨å…±æœ‰ {len(alias_df)} æ¡è®°å½•")
    return alias_df


def extract_unique_persons(alias_df):
    """
    ä»åˆ«åè¡¨æå–å”¯ä¸€çš„äººå‘˜
    ä½¿ç”¨ display_name ä½œä¸ºæ ‡å‡†åç§°
    """
    # æŒ‰ person_id åˆ†ç»„ï¼Œå–æ¯ç»„çš„ç¬¬ä¸€ä¸ª display_name
    unique_persons = alias_df.groupby('person_id').first().reset_index()
    unique_persons = unique_persons[['person_id', 'display_name']].copy()
    unique_persons.rename(columns={'display_name': 'person_name'}, inplace=True)
    
    logger.info(f"æå–åˆ° {len(unique_persons)} ä¸ªå”¯ä¸€äººå‘˜")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å
    name_counts = unique_persons['person_name'].value_counts()
    duplicates = name_counts[name_counts > 1]
    
    if not duplicates.empty:
        logger.warning("âš ï¸  å‘ç°é‡åçš„æƒ…å†µï¼š")
        for name, count in duplicates.items():
            logger.warning(f"  - {name}: {count}æ¬¡")
            # æ˜¾ç¤ºå…·ä½“çš„ person_id
            dup_persons = unique_persons[unique_persons['person_name'] == name]
            for _, row in dup_persons.iterrows():
                logger.warning(f"    â€¢ {row['person_id']}")
        logger.warning("  å»ºè®®åœ¨ alias è¡¨ä¸­ä¿®æ”¹ display_name ä»¥åŒºåˆ†é‡åäººå‘˜")
    else:
        logger.info("âœ… æ— é‡åæƒ…å†µ")
    
    return unique_persons


def read_existing_metadata(config):
    """è¯»å–ç°æœ‰çš„å…ƒæ•°æ®è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    metadata_config = config.get('volunteer_metadata_sheet')
    
    if not metadata_config:
        logger.info("å…ƒæ•°æ®è¡¨é…ç½®ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°è¡¨")
        return None
    
    try:
        logger.info(f"è¯»å–ç°æœ‰å…ƒæ•°æ®è¡¨: {metadata_config['url']}")
        
        # ä½¿ç”¨ GSheetClient
        client = GSheetClient()
        metadata_df = client.read_range(metadata_config['url'], metadata_config['range'])
        
        logger.info(f"ç°æœ‰å…ƒæ•°æ®è¡¨å…±æœ‰ {len(metadata_df)} æ¡è®°å½•")
        return metadata_df
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–ç°æœ‰å…ƒæ•°æ®è¡¨: {e}")
        logger.info("å°†åˆ›å»ºæ–°è¡¨")
        return None


def merge_metadata(unique_persons, existing_metadata):
    """
    åˆå¹¶æ–°æ—§æ•°æ®
    - ä¿ç•™ç°æœ‰çš„ family_group, unavailable_*, notes ç­‰åˆ—
    - æ›´æ–° person_name å’Œ updated_at
    - æ·»åŠ æ–°äººå‘˜
    """
    today = datetime.now().strftime('%Y-%m-%d')
    
    # åˆ›å»ºå®Œæ•´çš„åˆ—ç»“æ„
    columns = [
        'person_id',
        'person_name',
        'family_group',
        'unavailable_start',
        'unavailable_end',
        'unavailable_reason',
        'notes',
        'updated_at'
    ]
    
    if existing_metadata is None or existing_metadata.empty:
        # æ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œåˆ›å»ºæ–°çš„
        logger.info("åˆ›å»ºæ–°çš„å…ƒæ•°æ®è¡¨")
        new_metadata = unique_persons.copy()
        new_metadata['family_group'] = ''
        new_metadata['unavailable_start'] = ''
        new_metadata['unavailable_end'] = ''
        new_metadata['unavailable_reason'] = ''
        new_metadata['notes'] = ''
        new_metadata['updated_at'] = today
        
        # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
        new_metadata = new_metadata[columns]
        
        logger.info(f"ç”Ÿæˆ {len(new_metadata)} æ¡æ–°è®°å½•")
        return new_metadata
    
    # æœ‰ç°æœ‰æ•°æ®ï¼Œè¿›è¡Œåˆå¹¶
    logger.info("åˆå¹¶æ–°æ—§æ•°æ®...")
    
    # ç¡®ä¿ç°æœ‰æ•°æ®æœ‰æ‰€æœ‰å¿…è¦çš„åˆ—
    for col in columns:
        if col not in existing_metadata.columns:
            existing_metadata[col] = ''
    
    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨ç°æœ‰æ•°æ®
    existing_dict = {}
    for _, row in existing_metadata.iterrows():
        person_id = row['person_id']
        existing_dict[person_id] = {
            'family_group': row.get('family_group', ''),
            'unavailable_start': row.get('unavailable_start', ''),
            'unavailable_end': row.get('unavailable_end', ''),
            'unavailable_reason': row.get('unavailable_reason', ''),
            'notes': row.get('notes', ''),
        }
    
    # æ„å»ºæ–°çš„å…ƒæ•°æ®è¡¨
    new_records = []
    updated_count = 0
    new_count = 0
    
    for _, person in unique_persons.iterrows():
        person_id = person['person_id']
        person_name = person['person_name']
        
        if person_id in existing_dict:
            # ç°æœ‰äººå‘˜ï¼Œä¿ç•™å…¶ä»–åˆ—çš„æ•°æ®ï¼Œæ›´æ–° name å’Œ updated_at
            record = {
                'person_id': person_id,
                'person_name': person_name,  # æ›´æ–°åç§°
                **existing_dict[person_id],
                'updated_at': today  # æ›´æ–°æ—¶é—´
            }
            updated_count += 1
        else:
            # æ–°äººå‘˜
            record = {
                'person_id': person_id,
                'person_name': person_name,
                'family_group': '',
                'unavailable_start': '',
                'unavailable_end': '',
                'unavailable_reason': '',
                'notes': '',
                'updated_at': today
            }
            new_count += 1
        
        new_records.append(record)
    
    new_metadata = pd.DataFrame(new_records)
    new_metadata = new_metadata[columns]  # ç¡®ä¿åˆ—é¡ºåº
    
    logger.info(f"âœ… æ›´æ–°äº† {updated_count} æ¡ç°æœ‰è®°å½•")
    logger.info(f"âœ… æ·»åŠ äº† {new_count} æ¡æ–°è®°å½•")
    logger.info(f"âœ… æ€»å…± {len(new_metadata)} æ¡è®°å½•")
    
    return new_metadata


def write_metadata_to_sheet(config, metadata_df, dry_run=False):
    """å°†å…ƒæ•°æ®å†™å…¥ Google Sheets"""
    metadata_config = config.get('volunteer_metadata_sheet')
    
    if not metadata_config:
        raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ volunteer_metadata_sheet é…ç½®")
    
    if dry_run:
        logger.info("ğŸ” Dry-run æ¨¡å¼ï¼šä¸å†™å…¥ Google Sheets")
        logger.info("é¢„è§ˆå‰5æ¡è®°å½•ï¼š")
        print(metadata_df.head().to_string(index=False))
        return
    
    logger.info(f"å†™å…¥å…ƒæ•°æ®åˆ° Google Sheets: {metadata_config['url']}")
    
    # ä½¿ç”¨ GSheetClient
    client = GSheetClient()
    sheet_name = metadata_config['range'].split('!')[0]
    
    # å†™å…¥æ•°æ®ï¼ˆåŒ…å«è¡¨å¤´ï¼‰
    client.write_range(metadata_config['url'], f"{sheet_name}!A1", metadata_df)
    
    logger.info(f"âœ… æˆåŠŸå†™å…¥ {len(metadata_df)} æ¡è®°å½•åˆ° Google Sheets")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ä»åˆ«åè¡¨ç”ŸæˆåŒå·¥å…ƒæ•°æ®è¡¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å†™å…¥ï¼‰
  python scripts/generate_volunteer_metadata.py --dry-run
  
  # æ­£å¼å†™å…¥
  python scripts/generate_volunteer_metadata.py
  
  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  python scripts/generate_volunteer_metadata.py --config path/to/config.json
        """
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config/config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config/config.jsonï¼‰'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥ Google Sheets'
    )
    
    args = parser.parse_args()
    
    try:
        # 1. åŠ è½½é…ç½®
        logger.info("=" * 60)
        logger.info("ç”ŸæˆåŒå·¥å…ƒæ•°æ®è¡¨")
        logger.info("=" * 60)
        config = load_config(args.config)
        
        # 2. è¯»å–åˆ«åè¡¨
        alias_df = read_alias_data(config)
        
        # 3. æå–å”¯ä¸€äººå‘˜
        unique_persons = extract_unique_persons(alias_df)
        
        # 4. è¯»å–ç°æœ‰å…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        existing_metadata = read_existing_metadata(config)
        
        # 5. åˆå¹¶æ•°æ®
        metadata_df = merge_metadata(unique_persons, existing_metadata)
        
        # 6. å†™å…¥ Google Sheets
        write_metadata_to_sheet(config, metadata_df, dry_run=args.dry_run)
        
        # 7. æ‰“å°æ‘˜è¦
        logger.info("")
        logger.info("=" * 60)
        logger.info("âœ… å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"æ€»äººæ•°: {len(metadata_df)}")
        logger.info("")
        logger.info("ğŸ“ ä¸‹ä¸€æ­¥ï¼š")
        logger.info("  1. æ‰“å¼€ Google Sheets æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®")
        logger.info("  2. æ‰‹åŠ¨å¡«å†™ family_groupï¼ˆå®¶åº­å…³ç³»ï¼‰")
        logger.info("  3. æ‰‹åŠ¨å¡«å†™ unavailable_start/endï¼ˆä¸å¯ç”¨æ—¶é—´æ®µï¼‰")
        logger.info("  4. æ‰‹åŠ¨å¡«å†™ notesï¼ˆå¤‡æ³¨ä¿¡æ¯ï¼‰")
        logger.info("")
        logger.info(f"Google Sheets URL: {config.get('volunteer_metadata_sheet', {}).get('url', 'N/A')}")
        logger.info("=" * 60)
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == '__main__':
    sys.exit(main())

